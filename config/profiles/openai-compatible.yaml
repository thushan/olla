# OpenAI-compatible inference platform profile
name: openai-compatible
version: "1.0"
display_name: "OpenAI Compatible"
description: "OpenAI-compatible API"

# Routing configuration
routing:
  prefixes:
    - openai
    - openai-compatible

# API compatibility
api:
  openai_compatible: true
  paths:
    - /v1/models          # 0: health check & models
    - /v1/chat/completions # 1: chat completions
    - /v1/completions     # 2: completions
    - /v1/embeddings      # 3: embeddings
  model_discovery_path: /v1/models
  health_check_path: /v1/models

# Platform characteristics
characteristics:
  timeout: 2m
  max_concurrent_requests: 20
  default_priority: 50
  streaming_support: true

# Detection hints for auto-discovery
detection:
  path_indicators:
    - "/v1/models"

# Model handling
models:
  name_format: "{{.Name}}"
  capability_patterns:
    chat:
      - "gpt-*"
      - "*turbo*"
    embeddings:
      - "*embedding*"
      - "text-embedding-*"
    vision:
      - "*vision*"
      - "gpt-4-turbo*"
  # Context window patterns for cloud models
  context_patterns:
    - pattern: "gpt-5-thinking*"
      context: 400000   # API tier "thinking" variant
    - pattern: "gpt-5-main*"
      context: 128000   # API tier main variant
    - pattern: "gpt-5*"
      context: 32000    # Chat/consumer tier fallback
    - pattern: "gpt-4.1*"
      context: 1000000  # up to ~1M tokens (for GPT-4.1 family)
    - pattern: "gpt-4o*"
      context: 128000   # GPT-4o family
    - pattern: "gpt-4-turbo*"
      context: 128000   # GPT-4 Turbo
    - pattern: "gpt-4*"
      context: 8192     # legacy GPT-4
    - pattern: "gpt-3.5-turbo-16k*"
      context: 16384    # GPT-3.5 16k
    - pattern: "gpt-3.5-turbo*"
      context: 4096     # GPT-3.5 base
# Request/response handling
request:
  model_field_paths:
    - "model"
  response_format: "openai"
  parsing_rules:
    chat_completions_path: "/v1/chat/completions"
    completions_path: "/v1/completions"
    model_field_name: "model"
    supports_streaming: true

# Path indices for specific functions
path_indices:
  health: 0
  models: 0
  chat_completions: 1
  completions: 2
  embeddings: 3

# Resource management
resources:
  # Cloud-based models have no local resource requirements
  defaults:
    min_memory_gb: 0
    recommended_memory_gb: 0
    min_gpu_memory_gb: 0
    requires_gpu: false
    estimated_load_time_ms: 0

  # Cloud services handle their own concurrency
  concurrency_limits:
    - min_memory_gb: 0
      max_concurrent: 20  # Reasonable default for cloud APIs

  # No load time buffer needed for cloud services
  timeout_scaling:
    base_timeout_seconds: 120  # 2 minutes
    load_time_buffer: false

# Metrics extraction for OpenAI-compatible responses
metrics:
  extraction:
    enabled: true
    source: response_body
    format: json
    # OpenAI standard format
    paths:
      model: "$.model"
      finish_reason: "$.choices[0].finish_reason"  # String value (e.g., "stop", "length", "function_call")
      input_tokens: "$.usage.prompt_tokens"
      output_tokens: "$.usage.completion_tokens"
      total_tokens: "$.usage.total_tokens"
      # Some providers include additional metrics
      ttft_ms: "$.metrics.time_to_first_token"
      total_ms: "$.metrics.total_time"
    calculations:
      # Derive IsComplete from finish_reason presence (OpenAI doesn't have a separate 'done' field)
      is_complete: 'len(finish_reason) > 0'
      # Safe division: multiply first for precision, then divide with guard against zero
      tokens_per_second: "total_ms > 0 ? (output_tokens * 1000.0) / total_ms : 0"