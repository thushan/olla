# vLLM inference platform profile
name: vllm
home: "https://github.com/vllm-project/vllm"
version: "1.0"
display_name: "vLLM"
description: "vLLM high-performance inference server with PagedAttention"

# Routing configuration
routing:
  prefixes:
    - vllm

# API compatibility
api:
  openai_compatible: true
  paths:
    # Health and system endpoints
    - /health              # 0: health check (vLLM-specific endpoint)
    - /metrics             # 1: Prometheus metrics endpoint
    - /version             # 2: vLLM version information
    
    # Model management
    - /v1/models           # 3: list models (OpenAI-compatible)
    
    # Text generation endpoints (OpenAI-compatible)
    - /v1/chat/completions # 4: chat completions
    - /v1/completions      # 5: text completions
    
    # Embeddings and pooling
    - /v1/embeddings       # 6: embeddings/pooling API
    
    # Tokenization endpoints (vLLM-specific)
    - /tokenize            # 7: encode text to tokens
    - /detokenize          # 8: decode tokens to text
    - /v1/tokenize         # 9: versioned tokenize endpoint
    - /v1/detokenize       # 10: versioned detokenize endpoint
    
    # Reranking endpoints (compatible with Jina AI and Cohere)
    - /rerank              # 11: reranking API
    - /v1/rerank           # 12: versioned reranking API
    - /v2/rerank           # 13: v2 reranking API
    
    # Optional/debug endpoints
    - /get_tokenizer_info  # 14: tokenizer configuration info
    
  model_discovery_path: /v1/models
  health_check_path: /health
  metrics_path: /metrics

# Platform characteristics
characteristics:
  timeout: 2m
  max_concurrent_requests: 100
  default_priority: 80
  streaming_support: true

# Detection hints for auto-discovery
detection:
  path_indicators:
    - "/v1/models"
    - "/health"
    - "/metrics"        # Prometheus metrics endpoint is vLLM-specific
    - "/tokenize"       # vLLM-specific tokenization endpoint
  default_ports:
    - 8000
  response_headers:
    - "X-vLLM-Version"  # vLLM may add this in future versions

# Request/response handling
request:
  model_field_paths:
    - "model"
  response_format: "vllm"
  parsing_rules:
    chat_completions_path: "/v1/chat/completions"
    completions_path: "/v1/completions"
    model_field_name: "model"
    supports_streaming: true

# Path indices for specific functions
path_indices:
  health: 0
  metrics: 1
  version: 2
  models: 3
  chat_completions: 4
  completions: 5
  embeddings: 6
  tokenize: 7
  detokenize: 8
  v1_tokenize: 9
  v1_detokenize: 10
  rerank: 11
  v1_rerank: 12
  v2_rerank: 13
  tokenizer_info: 14

# Model handling
models:
  name_format: "{{.Name}}"  # vLLM uses full model names like "meta-llama/Meta-Llama-3.1-8B-Instruct"
  capability_patterns:
    chat:
      - "*-Chat-*"
      - "*-Instruct*"
      - "*-chat-*"
    embeddings:
      - "*embedding*"
      - "*-embed-*"
    vision:
      - "*vision*"
      - "*llava*"
    code:
      - "*code*"
      - "*Code*"
  # Context window patterns for common vLLM models
  context_patterns:
    - pattern: "*llama-3.1*"
      context: 131072
    - pattern: "*llama-3*"
      context: 8192
    - pattern: "*mistral*"
      context: 32768
    - pattern: "*mixtral*"
      context: 32768
    - pattern: "*gemma-2*"
      context: 8192
    - pattern: "*tinyllama*"
      context: 2048

# Resource management - vLLM is optimised for high throughput
resources:
  # Model size patterns for vLLM deployments
  model_sizes:
    - patterns: ["*70b*", "*72b*"]
      min_memory_gb: 140  # vLLM requires more memory for KV cache
      recommended_memory_gb: 160
      min_gpu_memory_gb: 140
      estimated_load_time_ms: 60000
    - patterns: ["*34b*", "*33b*", "*30b*"]
      min_memory_gb: 70
      recommended_memory_gb: 80
      min_gpu_memory_gb: 70
      estimated_load_time_ms: 45000
    - patterns: ["*13b*", "*14b*"]
      min_memory_gb: 30
      recommended_memory_gb: 40
      min_gpu_memory_gb: 30
      estimated_load_time_ms: 30000
    - patterns: ["*7b*", "*8b*"]
      min_memory_gb: 16
      recommended_memory_gb: 24
      min_gpu_memory_gb: 16
      estimated_load_time_ms: 20000
    - patterns: ["*3b*"]
      min_memory_gb: 8
      recommended_memory_gb: 12
      min_gpu_memory_gb: 8
      estimated_load_time_ms: 15000
    - patterns: ["*1b*", "*1.1b*", "*1.5b*"]
      min_memory_gb: 4
      recommended_memory_gb: 8
      min_gpu_memory_gb: 4
      estimated_load_time_ms: 10000
      
  defaults:
    min_memory_gb: 8
    recommended_memory_gb: 16
    min_gpu_memory_gb: 8
    requires_gpu: true
    estimated_load_time_ms: 30000
    
  # vLLM supports high concurrency with PagedAttention
  concurrency_limits:
    - min_memory_gb: 100  # 70B+ models
      max_concurrent: 10
    - min_memory_gb: 50   # 30B+ models
      max_concurrent: 20
    - min_memory_gb: 20   # 13B+ models
      max_concurrent: 50
    - min_memory_gb: 0    # smaller models
      max_concurrent: 100
      
  # Timeout scaling for vLLM
  timeout_scaling:
    base_timeout_seconds: 120
    load_time_buffer: true

# vLLM-specific features
features:
  # Prometheus metrics exposed at /metrics
  metrics:
    enabled: true
    prefix: "vllm:"
    categories:
      - system        # num_requests_running, num_requests_waiting, gpu_cache_usage_perc
      - cache         # cpu_prefix_cache_hit_rate, gpu_prefix_cache_hit_rate
      - tokens        # prompt_tokens_total, generation_tokens_total
      - performance   # e2e_request_latency_seconds, time_to_first_token_seconds
      - speculative   # spec_decode_num_accepted_tokens_total (if enabled)
  
  # Tokenization API support
  tokenization:
    enabled: true
    endpoints:
      - /tokenize       # HuggingFace tokenizer.encode() wrapper
      - /detokenize     # HuggingFace tokenizer.decode() wrapper
      - /v1/tokenize    # Versioned API
      - /v1/detokenize  # Versioned API
  
  # Reranking support (Jina AI & Cohere compatible)
  reranking:
    enabled: true
    compatible_with:
      - jina_ai
      - cohere
  
  # PagedAttention optimisation
  paged_attention:
    enabled: true
    description: "Memory-efficient attention mechanism for high throughput"
  
  # Continuous batching
  continuous_batching:
    enabled: true
    description: "Dynamic batching for optimal GPU utilisation"

# Metrics extraction for vLLM responses
metrics:
  extraction:
    enabled: true
    source: response_body
    format: json
    # vLLM uses OpenAI-compatible format for chat/completions endpoints
    paths:
      model: "$.model"
      finish_reason: "$.choices[0].finish_reason"  # String value (e.g., "stop", "length")
      input_tokens: "$.usage.prompt_tokens"
      output_tokens: "$.usage.completion_tokens"
      total_tokens: "$.usage.total_tokens"
      # vLLM may include additional performance metrics
      ttft_ms: "$.metrics.time_to_first_token_ms"
      generation_time_ms: "$.metrics.generation_time_ms"
    calculations:
      # Derive IsComplete from finish_reason presence (vLLM doesn't have a separate 'done' field)
      is_complete: 'len(finish_reason) > 0'
      # Safe division: multiply first for precision, then divide with guard against zero
      tokens_per_second: "generation_time_ms > 0 ? (output_tokens * 1000.0) / generation_time_ms : 0"