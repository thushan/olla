# Ollama inference platform profile
name: ollama
version: "1.0"
display_name: "Ollama"
description: "Local Ollama instance for running GGUF models"

# Routing configuration
routing:
  prefixes:
    - ollama

# API compatibility
api:
  openai_compatible: true

  # Anthropic Messages API support (v0.14.0+)
  # UNSUPPORTED:
  #           - /v1/messages/count_tokens
  #             [11-01-2026]: https://docs.ollama.com/api/anthropic-compatibility#not-supported
  anthropic_support:
    enabled: true
    messages_path: /v1/messages
    token_count: false
    min_version: "0.14.0"
    limitations:
      - token_counting_404

  paths:
    - /                    # 0: health check
    - /api/generate        # 1: text completion
    - /api/chat            # 2: chat completion  
    - /api/embeddings      # 3: generate embeddings
    - /api/tags            # 4: list local models
    - /api/show            # 5: show model info
    - /v1/models           # 6: OpenAI compat
    - /v1/chat/completions # 7: OpenAI compat
    - /v1/completions      # 8: OpenAI compat
    - /v1/embeddings       # 9: OpenAI compat
  model_discovery_path: /api/tags
  health_check_path: /

# Platform characteristics
characteristics:
  timeout: 5m  # Ollama can be slow for large models
  max_concurrent_requests: 10
  default_priority: 100
  streaming_support: true
  
# Detection hints for auto-discovery
detection:
  user_agent_patterns:
    - "ollama/"
  headers:
    - "X-ProfileOllama-Version"
  path_indicators:
    - "/"
    - "/api/tags"
  default_ports:
    - 11434

# Model handling
models:
  name_format: "{{.Name}}"  # e.g., "llama3:latest"
  capability_patterns:
    vision:
      - "*llava*"
      - "*vision*"
      - "*bakllava*"
    embeddings:
      - "*embed*"
      - "nomic-embed-text"
      - "mxbai-embed-large"
    code:
      - "*code*"
      - "codellama*"
      - "deepseek-coder*"
      - "qwen*coder*"
  # Context window detection patterns
  context_patterns:
    - pattern: "*-32k*"
      context: 32768
    - pattern: "*-16k*"
      context: 16384
    - pattern: "*-8k*"
      context: 8192
    - pattern: "*:32k*"
      context: 32768
    - pattern: "*:16k*"
      context: 16384
    - pattern: "*:8k*"
      context: 8192
    - pattern: "llama3*"
      context: 8192
    - pattern: "llama-3*"
      context: 8192
      
# Request/response handling
request:
  model_field_paths:
    - "model"
  response_format: "ollama"
  parsing_rules:
    chat_completions_path: "/api/chat"
    completions_path: "/api/generate"
    generate_path: "/api/generate"
    model_field_name: "model"
    supports_streaming: true

# Path indices for specific functions
path_indices:
  health: 0
  completions: 1
  chat_completions: 2
  embeddings: 3
  models: 4

# Resource management
resources:
  # Model size patterns with memory requirements
  model_sizes:
    - patterns: ["70b", "72b"]
      min_memory_gb: 40
      recommended_memory_gb: 48
      min_gpu_memory_gb: 40
      estimated_load_time_ms: 300000  # 5 minutes
    - patterns: ["65b"]
      min_memory_gb: 35
      recommended_memory_gb: 40
      min_gpu_memory_gb: 35
      estimated_load_time_ms: 240000  # 4 minutes
    - patterns: ["34b", "33b", "30b"]
      min_memory_gb: 20
      recommended_memory_gb: 24
      min_gpu_memory_gb: 20
      estimated_load_time_ms: 120000  # 2 minutes
    - patterns: ["13b", "14b"]
      min_memory_gb: 10
      recommended_memory_gb: 16
      min_gpu_memory_gb: 10
      estimated_load_time_ms: 60000   # 1 minute
    - patterns: ["7b", "8b"]
      min_memory_gb: 6
      recommended_memory_gb: 8
      min_gpu_memory_gb: 6
      estimated_load_time_ms: 30000   # 30 seconds
    - patterns: ["3b"]
      min_memory_gb: 3
      recommended_memory_gb: 4
      min_gpu_memory_gb: 3
      estimated_load_time_ms: 15000   # 15 seconds
    - patterns: ["1b", "1.5b"]
      min_memory_gb: 2
      recommended_memory_gb: 3
      min_gpu_memory_gb: 2
      estimated_load_time_ms: 10000   # 10 seconds
  
  # Quantization multipliers
  quantization:
    multipliers:
      q4: 0.5      # 4-bit quantization ~50% memory reduction
      q5: 0.625    # 5-bit quantization ~37.5% memory reduction
      q6: 0.75     # 6-bit quantization ~25% memory reduction
      q8: 0.875    # 8-bit quantization ~12.5% memory reduction
  
  # Default resource requirements
  defaults:
    min_memory_gb: 4
    recommended_memory_gb: 8
    min_gpu_memory_gb: 4
    requires_gpu: false
    estimated_load_time_ms: 5000
  
  # Dynamic concurrency limits based on model size
  concurrency_limits:
    - min_memory_gb: 30   # 70B+ models
      max_concurrent: 1
    - min_memory_gb: 15   # 30B+ models
      max_concurrent: 2
    - min_memory_gb: 8    # 13B+ models
      max_concurrent: 4
    - min_memory_gb: 0    # smaller models
      max_concurrent: 8
  
  # Dynamic timeout scaling
  timeout_scaling:
    base_timeout_seconds: 30
    load_time_buffer: true  # adds estimated_load_time_ms to timeout

# Metrics extraction from Ollama responses
metrics:
  extraction:
    enabled: true
    source: "response_body"
    format: "json"
    
    # JSONPath expressions for extracting values from Ollama response
    paths:
      model: "$.model"
      is_complete: "$.done"  # Ollama provides 'done' as a boolean directly
      finish_reason: "$.finish_reason"  # Optional: Ollama may include this in some versions
      # Token counts
      input_tokens: "$.prompt_eval_count"
      output_tokens: "$.eval_count"
      # Timing data (in nanoseconds from Ollama)
      total_duration_ns: "$.total_duration"
      load_duration_ns: "$.load_duration"
      prompt_duration_ns: "$.prompt_eval_duration"
      eval_duration_ns: "$.eval_duration"
      
    # Simple calculations to convert to useful metrics
    calculations:
      # Safe division: multiply first for precision, then divide with guard against zero
      tokens_per_second: "eval_duration_ns > 0 ? (output_tokens * 1000000000.0) / eval_duration_ns : 0"
      ttft_ms: "prompt_duration_ns / 1000000"
      total_ms: "total_duration_ns / 1000000"
      model_load_ms: "load_duration_ns / 1000000"