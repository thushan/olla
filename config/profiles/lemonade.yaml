# Lemonade SDK inference platform profile
name: lemonade
home: "https://lemonade-server.ai"
version: "1.0"
display_name: "Lemonade"
description: "Lemonade local LLM serving platform with AMD Ryzen AI optimisation"

# Routing configuration
routing:
  prefixes:
    - lemonade

# API compatibility
api:
  openai_compatible: true
  paths:
    # Health and system endpoints
    - /api/v1/health      # 0: health check
    - /api/v1/stats       # 1: runtime statistics
    - /api/v1/system-info # 2: system information

    # Model management (OpenAI-compatible)
    - /api/v1/models      # 3: list models

    # Text generation endpoints (OpenAI-compatible)
    - /api/v1/chat/completions # 4: chat completions
    - /api/v1/completions      # 5: text completions
    - /api/v1/embeddings       # 6: embeddings (may not work on all recipes)

    # Model lifecycle management (Lemonade-specific)
    - /api/v1/pull        # 7: download/install models
    - /api/v1/load        # 8: load models into memory
    - /api/v1/unload      # 9: unload models from memory
    - /api/v1/delete      # 10: delete models from disk

  model_discovery_path: /api/v1/models
  health_check_path: /api/v1/health
  stats_path: /api/v1/stats

# Platform characteristics
characteristics:
  timeout: 2m
  max_concurrent_requests: 100  # Conservative for local inference focus
  default_priority: 75          # Lower than cloud solutions due to local focus
  streaming_support: true

# Detection hints for auto-discovery
detection:
  path_indicators:
    - "/api/v1/models"
    - "/api/v1/health"
    - "/api/v1/system-info"  # Lemonade-specific
  default_ports:
    - 8000
  response_headers:
    - "X-Lemonade-Version"

# Request/response handling
request:
  model_field_paths:
    - "model"
  response_format: "lemonade"
  parsing_rules:
    chat_completions_path: "/api/v1/chat/completions"
    completions_path: "/api/v1/completions"
    model_field_name: "model"
    supports_streaming: true

# Path indices for specific functions
path_indices:
  health: 0
  stats: 1
  system_info: 2
  models: 3
  chat_completions: 4
  completions: 5
  embeddings: 6
  pull: 7
  load: 8
  unload: 9
  delete: 10

# Model handling
models:
  name_format: "{{.Name}}"  # Lemonade uses friendly names like "Qwen2.5-0.5B-Instruct-CPU"
  capability_patterns:
    chat:
      - "*-Instruct-*"
      - "*-Chat-*"
      - "*-it-*"
    embeddings:
      - "*embed*"
    code:
      - "*code*"
      - "*Coder*"
      - "*Devstral*"
    vision:
      - "*VL-*"
      - "*vision*"
      - "*Scout*"
    reasoning:
      - "*DeepSeek-R1*"
      - "*Cogito*"
    reranking:
      - "*reranker*"
  # Context window patterns for common local models
  context_patterns:
    - pattern: "*llama-3.1*"
      context: 131072
    - pattern: "*llama-3.2*"
      context: 131072
    - pattern: "*llama-3*"
      context: 8192
    - pattern: "*mistral*"
      context: 32768
    - pattern: "*phi*"
      context: 4096
    - pattern: "*qwen*"
      context: 32768
    - pattern: "*gemma*"
      context: 8192

# Lemonade-specific features
features:
  # Local inference optimisation
  local_inference:
    enabled: true
    description: "Optimised for local deployment scenarios"

  # Multi-hardware support
  hardware_acceleration:
    enabled: true
    description: "Support for CPU, GPU, and NPU inference"
    supported_types:
      - "CPU"
      - "GPU"
      - "NPU"
      - "AMD_Ryzen_AI"

  # Multiple inference backends (identified by recipe field)
  backends:
    enabled: true
    description: "Multiple inference engines for different hardware"
    supported_engines:
      - "oga-cpu"      # ONNX Runtime for CPU
      - "oga-npu"      # ONNX Runtime for NPU
      - "oga-igpu"     # ONNX Runtime for iGPU (DirectML)
      - "llamacpp"     # llama.cpp for GGUF models
      - "flm"          # Fast Language Models

  # Model format support (inferred from recipe)
  model_formats:
    enabled: true
    description: "Support for multiple model formats"
    supported_formats:
      - "GGUF"         # llamacpp recipe
      - "ONNX"         # oga-* recipes

  # Model management
  model_management:
    enabled: true
    description: "Rich model lifecycle management"
    capabilities:
      - "pull"         # Download models
      - "load"         # Load into memory
      - "unload"       # Unload from memory
      - "delete"       # Remove from disk

  # Cross-platform support
  cross_platform:
    enabled: true
    description: "Support for Windows, Linux, and macOS"
    platforms:
      - "Windows"
      - "Linux"
      - "macOS"

# Resource management - Lemonade local optimisations
resources:
  # Lemonade memory requirements (optimised for local scenarios)
  model_sizes:
    - patterns: ["*70b*", "*72b*", "*120b*"]
      min_memory_gb: 64
      recommended_memory_gb: 96
      min_gpu_memory_gb: 48
      estimated_load_time_ms: 60000
    - patterns: ["*30b*", "*32b*", "*33b*"]
      min_memory_gb: 32
      recommended_memory_gb: 48
      min_gpu_memory_gb: 24
      estimated_load_time_ms: 45000
    - patterns: ["*13b*", "*14b*", "*20b*"]
      min_memory_gb: 16
      recommended_memory_gb: 32
      min_gpu_memory_gb: 16
      estimated_load_time_ms: 30000
    - patterns: ["*7b*", "*8b*"]
      min_memory_gb: 8
      recommended_memory_gb: 16
      min_gpu_memory_gb: 8
      estimated_load_time_ms: 20000
    - patterns: ["*3b*", "*4b*"]
      min_memory_gb: 4
      recommended_memory_gb: 8
      min_gpu_memory_gb: 4
      estimated_load_time_ms: 15000
    - patterns: ["*0.5b*", "*0.6b*", "*1b*", "*1.5b*", "*1.7b*"]
      min_memory_gb: 2
      recommended_memory_gb: 4
      min_gpu_memory_gb: 2
      estimated_load_time_ms: 10000

  defaults:
    min_memory_gb: 4
    recommended_memory_gb: 8
    min_gpu_memory_gb: 4
    requires_gpu: false     # CPU inference supported
    estimated_load_time_ms: 25000

  # Lemonade concurrency limits (conservative for local)
  concurrency_limits:
    - min_memory_gb: 100
      max_concurrent: 10
    - min_memory_gb: 50
      max_concurrent: 20
    - min_memory_gb: 20
      max_concurrent: 50
    - min_memory_gb: 0
      max_concurrent: 100

  # Timeout scaling for Lemonade
  timeout_scaling:
    base_timeout_seconds: 120
    load_time_buffer: true

# Metrics extraction for Lemonade responses
metrics:
  extraction:
    enabled: true
    source: response_body
    format: json
    # Lemonade uses OpenAI-compatible format
    paths:
      # We get back hf checkpoint path instead of model name due to Lemonade design.
      # Eg. "model": "amd/Qwen2.5-0.5B-Instruct-quantized_int4-float16-cpu-onnx"
      # vs "Qwen2.5-0.5B-Instruct-CPU"
      model: "$.model"
      finish_reason: "$.choices[0].finish_reason"
      input_tokens: "$.usage.prompt_tokens"
      output_tokens: "$.usage.completion_tokens"
      total_tokens: "$.usage.total_tokens"
    calculations:
      is_complete: 'len(finish_reason) > 0'
      tokens_per_second: "output_tokens > 0 && response_time_ms > 0 ? (output_tokens * 1000.0) / response_time_ms : 0"

# Security considerations (Lemonade is local-only)
security:
  local_only: true
  description: "Lemonade is intended for local systems only - do not expose to internet"
  recommendations:
    - "Keep server port (8000) local only"
    - "Use firewall rules to prevent external access"
    - "Monitor for unusual network activity"
