# LM Studio inference platform profile
name: lm-studio
version: "1.0"
display_name: "LM Studio"
description: "LM Studio local inference server"

# Routing configuration - support all variations
routing:
  prefixes:
    - lmstudio
    - lm-studio
    - lm_studio

# API compatibility
api:
  openai_compatible: true
  paths:
    - /v1/models          # 0: health check & models
    - /v1/chat/completions # 1: chat completions
    - /v1/completions     # 2: completions
    - /v1/embeddings      # 3: embeddings
    - /api/v0/models      # 4: legacy models endpoint
  model_discovery_path: /api/v0/models
  health_check_path: /v1/models

# Platform characteristics
characteristics:
  timeout: 3m
  max_concurrent_requests: 1  # LM Studio typically handles one at a time
  default_priority: 90
  streaming_support: true

# Detection hints for auto-discovery
detection:
  path_indicators:
    - "/v1/models"
    - "/api/v0/models"
  default_ports:
    - 1234

# Model handling
models:
  name_format: "{{.Name}}"
  capability_patterns:
    chat:
      - "*"  # All models support chat in LM Studio
  # Context window detection patterns
  context_patterns:
    - pattern: "*-32k*"
      context: 32768
    - pattern: "*-16k*"
      context: 16384
    - pattern: "*-8k*"
      context: 8192
    - pattern: "*:32k*"
      context: 32768
    - pattern: "*:16k*"
      context: 16384
    - pattern: "*:8k*"
      context: 8192
    - pattern: "llama3*"
      context: 8192
    - pattern: "llama-3*"
      context: 8192

# Request/response handling
request:
  model_field_paths:
    - "model"
  response_format: "lmstudio"
  parsing_rules:
    chat_completions_path: "/v1/chat/completions"
    completions_path: "/v1/completions"
    model_field_name: "model"
    supports_streaming: true

# Path indices for specific functions  
path_indices:
  health: 0
  models: 0
  chat_completions: 1
  completions: 2
  embeddings: 3

# Resource management
resources:
  # Model size patterns - LM Studio shows exact sizes
  model_sizes:
    - patterns: ["70b", "72b"]
      min_memory_gb: 42      # Base memory * 0.6 for quantization
      recommended_memory_gb: 52.5   # Base memory * 0.75
      min_gpu_memory_gb: 42
      estimated_load_time_ms: 1000  # LM Studio preloads
    - patterns: ["65b"]
      min_memory_gb: 39
      recommended_memory_gb: 48.75
      min_gpu_memory_gb: 39
      estimated_load_time_ms: 1000
    - patterns: ["34b", "33b"]
      min_memory_gb: 20.4
      recommended_memory_gb: 25.5
      min_gpu_memory_gb: 20.4
      estimated_load_time_ms: 1000
    - patterns: ["13b", "14b"]
      min_memory_gb: 8.4
      recommended_memory_gb: 10.5
      min_gpu_memory_gb: 8.4
      estimated_load_time_ms: 1000
    - patterns: ["7b", "8b"]
      min_memory_gb: 4.8
      recommended_memory_gb: 6
      min_gpu_memory_gb: 4.8
      estimated_load_time_ms: 1000
    - patterns: ["3b"]
      min_memory_gb: 1.8
      recommended_memory_gb: 2.25
      min_gpu_memory_gb: 1.8
      estimated_load_time_ms: 1000
  
  # Default resource requirements
  defaults:
    min_memory_gb: 4.2      # 7B model * 0.6
    recommended_memory_gb: 5.25  # 7B model * 0.75
    min_gpu_memory_gb: 4.2
    requires_gpu: false
    estimated_load_time_ms: 1000
  
  # LM Studio typically handles one model at a time
  concurrency_limits:
    - min_memory_gb: 0
      max_concurrent: 1
  
  # No need for load time buffer - models are preloaded
  timeout_scaling:
    base_timeout_seconds: 180  # 3 minutes
    load_time_buffer: false