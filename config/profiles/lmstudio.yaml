# LM Studio inference platform profile
name: lm-studio
version: "1.0"
display_name: "LM Studio"
description: "LM Studio local inference server"

# Routing configuration - support all variations
routing:
  prefixes:
    - lmstudio
    - lm-studio
    - lm_studio

# API compatibility
api:
  openai_compatible: true

  # Anthropic Messages API support (v0.4.1+)
  # Added specifically for Claude Code integration, enabling native Anthropic API support
  # without requiring translation middleware
  anthropic_support:
    enabled: true
    messages_path: /v1/messages
    token_count: false
    min_version: "0.4.1"

  paths:
    - /v1/models          # 0: health check & models
    - /v1/chat/completions # 1: chat completions
    - /v1/completions     # 2: completions
    - /v1/embeddings      # 3: embeddings
    - /api/v0/models      # 4: legacy models endpoint
  model_discovery_path: /api/v0/models
  health_check_path: /v1/models

# Platform characteristics
characteristics:
  timeout: 3m
  max_concurrent_requests: 1  # LM Studio typically handles one at a time
  default_priority: 90
  streaming_support: true

# Detection hints for auto-discovery
detection:
  path_indicators:
    - "/v1/models"
    - "/api/v0/models"
  default_ports:
    - 1234

# Model handling
models:
  name_format: "{{.Name}}"
  capability_patterns:
    chat:
      - "*"  # All models support chat in LM Studio
  # Context window detection patterns
  context_patterns:
    - pattern: "*-32k*"
      context: 32768
    - pattern: "*-16k*"
      context: 16384
    - pattern: "*-8k*"
      context: 8192
    - pattern: "*:32k*"
      context: 32768
    - pattern: "*:16k*"
      context: 16384
    - pattern: "*:8k*"
      context: 8192
    - pattern: "llama3*"
      context: 8192
    - pattern: "llama-3*"
      context: 8192

# Request/response handling
request:
  model_field_paths:
    - "model"
  response_format: "lmstudio"
  parsing_rules:
    chat_completions_path: "/v1/chat/completions"
    completions_path: "/v1/completions"
    model_field_name: "model"
    supports_streaming: true

# Path indices for specific functions  
path_indices:
  health: 0
  models: 0
  chat_completions: 1
  completions: 2
  embeddings: 3

# Resource management
resources:
  # Model size patterns - LM Studio shows exact sizes
  model_sizes:
    - patterns: ["70b", "72b"]
      min_memory_gb: 42      # Base memory * 0.6 for quantization
      recommended_memory_gb: 52.5   # Base memory * 0.75
      min_gpu_memory_gb: 42
      estimated_load_time_ms: 1000  # LM Studio preloads
    - patterns: ["65b"]
      min_memory_gb: 39
      recommended_memory_gb: 48.75
      min_gpu_memory_gb: 39
      estimated_load_time_ms: 1000
    - patterns: ["34b", "33b"]
      min_memory_gb: 20.4
      recommended_memory_gb: 25.5
      min_gpu_memory_gb: 20.4
      estimated_load_time_ms: 1000
    - patterns: ["13b", "14b"]
      min_memory_gb: 8.4
      recommended_memory_gb: 10.5
      min_gpu_memory_gb: 8.4
      estimated_load_time_ms: 1000
    - patterns: ["7b", "8b"]
      min_memory_gb: 4.8
      recommended_memory_gb: 6
      min_gpu_memory_gb: 4.8
      estimated_load_time_ms: 1000
    - patterns: ["3b"]
      min_memory_gb: 1.8
      recommended_memory_gb: 2.25
      min_gpu_memory_gb: 1.8
      estimated_load_time_ms: 1000
  
  # Default resource requirements
  defaults:
    min_memory_gb: 4.2      # 7B model * 0.6
    recommended_memory_gb: 5.25  # 7B model * 0.75
    min_gpu_memory_gb: 4.2
    requires_gpu: false
    estimated_load_time_ms: 1000
  
  # LM Studio typically handles one model at a time
  concurrency_limits:
    - min_memory_gb: 0
      max_concurrent: 1
  
  # No need for load time buffer - models are preloaded
  timeout_scaling:
    base_timeout_seconds: 180  # 3 minutes
    load_time_buffer: false

# Metrics extraction for LM Studio responses
metrics:
  extraction:
    enabled: true
    source: response_body
    format: json
    # LM Studio uses OpenAI-compatible format
    paths:
      model: "$.model"
      finish_reason: "$.choices[0].finish_reason"  # String value (e.g., "stop", "length")
      input_tokens: "$.usage.prompt_tokens"
      output_tokens: "$.usage.completion_tokens"
      total_tokens: "$.usage.total_tokens"
    calculations:
      # Derive IsComplete from finish_reason presence (LM Studio doesn't have a separate 'done' field)
      is_complete: 'len(finish_reason) > 0'
      # LM Studio doesn't provide timing data, so we can't calculate tokens/sec