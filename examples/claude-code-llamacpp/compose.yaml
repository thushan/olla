services:
  llama-cpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llama-cpp
    restart: unless-stopped
    ports:
      - "8080:8080"
    volumes:
      - ./models:/models:ro          # Mount models directory (read-only)
    command:
      - "--model"
      - "/models/Llama-3.4-3B-Instruct-Q5_K_M.gguf"  # UPDATE THIS with your model filename
      - "--ctx-size"
      - "8192"                       # Context window
      - "--n-gpu-layers"
      - "0"                          # CPU-only (set > 0 for GPU)
      - "--threads"
      - "8"                          # CPU threads (adjust for your CPU)
      - "--batch-size"
      - "512"
      - "--port"
      - "8080"
      - "--host"
      - "0.0.0.0"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s              # Give time for model loading
    # Uncomment for NVIDIA GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  olla:
    image: ghcr.io/thushan/olla:latest
    container_name: olla
    restart: unless-stopped
    ports:
      - "40114:40114"
    volumes:
      - ./olla.yaml:/app/config.yaml:ro
      - ./logs:/app/logs
    depends_on:
      llama-cpp:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:40114/internal/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    environment:
      - LOG_LEVEL=info
