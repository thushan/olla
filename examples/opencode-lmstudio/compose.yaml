services:
  # Olla proxy - connects to LM Studio running on host machine
  olla:
    image: ghcr.io/thushan/olla:latest
    container_name: olla
    restart: unless-stopped
    ports:
      - "40114:40114"
    volumes:
      # Override the default with our custom configuration
      - ./olla.yaml:/app/config.yaml:ro
      - ./.container/olla/logs:/app/logs
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:40114/internal/health"]
      timeout: 5s
      interval: 30s
      retries: 3
      start_period: 10s
    # Note: LM Studio runs on the host machine, not in a container
    # The olla.yaml configuration uses host.docker.internal (Mac/Windows)
    # or 172.17.0.1 (Linux) to reach the host

networks:
  default:
    name: olla-opencode-network
